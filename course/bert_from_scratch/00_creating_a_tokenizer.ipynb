{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install HuggingFace `datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use *conda*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c huggingface -c conda-forge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this doesn't work, install from source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/datasets.git\n",
    "!cd datsets\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "source": [
    "We then load the **Italian** part of the [**OSCAR**](https://huggingface.co/datasets/oscar) dataset. This is a *huge* dataset so download can take a long time:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset oscar (C:\\Users\\James\\.cache\\huggingface\\datasets\\oscar\\unshuffled_deduplicated_it\\1.0.0\\e4f06cecc7ae02f7adf85640b4019bf476d44453f251a1d84aebae28b0f8d51d)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text'],\n",
       "        num_rows: 28522082\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28522082"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 28522082\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': \"La estrazione numero 48 del 10 e LOTTO ogni 5 minuti e' avvenuta sabato 15 settembre 2018 alle ore 04:00 a Roma, nel Centro Elaborazione Dati della Lottomatica Italia (ora GTech SpA), con la supervisione della Amministrazione Autonoma dei Monopoli di Stato (AAMS), incaricata di vigilare sulla regolarità delle operazioni di sorteggio.\\nIl Montepremi della 48ª estrazione viene ripartito tra i vincitori delle singole categorie di premio.\\nRicorda di controllare il Numero ORO 53. E, se lo hai giocato, anche il DOPPIO ORO 53 e 66. Se indovini puoi vincere premi più ricchi.\\nIl nostro sito web impiega cookies per migliorare la navigazione del visitatore. L’utente è consapevole che, continuando a visitare il nostro sito web, accetta l’utilizzo dei cookies Accetto Informazioni\\n(C) Copyright 2013-2017 10elotto.biz | Il presente sito è da considerarsi un sito indipendente, NON collegato alla rete ufficiale Gtech SpA.\"}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "source": [
    "Now we save this data to file as several *plaintext* files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 28522082/28522082 [33:32<00:00, 14173.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 10_000:\n",
    "        # once we git the 10K mark, save to file\n",
    "        with open(f'../../data/text/oscar_it/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'../../data/text/oscar_it/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "source": [
    "Now we get a list of paths to each file in our *oscar_it* directory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['..\\\\..\\\\data\\\\text\\\\oscar_it\\\\text_995.txt',\n",
       " '..\\\\..\\\\data\\\\text\\\\oscar_it\\\\text_996.txt',\n",
       " '..\\\\..\\\\data\\\\text\\\\oscar_it\\\\text_997.txt',\n",
       " '..\\\\..\\\\data\\\\text\\\\oscar_it\\\\text_998.txt',\n",
       " '..\\\\..\\\\data\\\\text\\\\oscar_it\\\\text_999.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path('../../data/text/oscar_it').glob('**/*.txt')]\n",
    "\n",
    "paths[-5:]"
   ]
  },
  {
   "source": [
    "Now we move onto training the tokenizer. We use a byte-level Byte-pair encoding (BPE) tokenizer. This allows us to build the vocabulary from an alphabet of single bytes, meaning all words will be decomposable into tokens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths[:5], vocab_size=30_522, min_frequency=2,\n",
    "                special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]'])"
   ]
  },
  {
   "source": [
    "We can now save our tokenizer to file, we'll be giving our model a traditional Italian name - filiBERTo:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./filiberto\\\\filiberto-vocab.json', './filiberto\\\\filiberto-merges.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('./filiberto')\n",
    "\n",
    "tokenizer.save_model('./filiberto', 'filiberto')"
   ]
  },
  {
   "source": [
    "Now we have two files that outline our new filiBERTo tokenizer:\n",
    "\n",
    "* the *vocab.json* - a mapping file between tokens to token IDs\n",
    "\n",
    "* and *merges.txt* - which describes which characters/set of characters can be decomposed/composed smaller/larger tokens\n",
    "\n",
    "To begin using our tokenizer like we would usually use a `from_pretrained` tokenizer we do this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    './filiberto/filiberto-vocab.json',\n",
    "    './filiberto/filiberto-merges.txt'\n",
    ")\n",
    "\n",
    "# set [CLS] and [SEP] to be added to start-end of sequences\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    ('[SEP]', tokenizer.token_to_id('[SEP]')),\n",
    "    ('[CLS]', tokenizer.token_to_id('[CLS]'))\n",
    ")\n",
    "\n",
    "# truncate anything more than 512 characters in length\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# and enable padding to 512 too\n",
    "tokenizer.enable_padding(length=512, pad_token='[PAD]')\n",
    "\n",
    "# test our tokenizer on a simple sentence\n",
    "tokens = tokenizer.encode('ciao, come va?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]', 'ciao', ',', 'Ġcome', 'Ġva', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tokens.tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 16834, 16, 488, 611, 35, 2, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tokens.ids[:10]"
   ]
  },
  {
   "source": [
    "We can see here that our **CLS** token is now placed at the beginning of our sequences using token ID *1*. At the end of the sequence we see the **SEP** token represented by *2*. Following this we have our **PAD** tokens which pad each sequence upto a length of *512*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}