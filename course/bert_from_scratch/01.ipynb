{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "ml",
   "display_name": "ML",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "First we will initialize our filiBERTo tokenizer like before, and use it to encode our data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    './filiberto/filiberto-vocab.json',\n",
    "    './filiberto/filiberto-merges.txt'\n",
    ")\n",
    "# set [CLS] and [SEP] to be added to start-end of sequences\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    ('[SEP]', tokenizer.token_to_id('[SEP]')),\n",
    "    ('[CLS]', tokenizer.token_to_id('[CLS]'))\n",
    ")\n",
    "# truncate anything more than 512 tokens in length\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# and enable padding to 512 too\n",
    "tokenizer.enable_padding(length=512, pad_token='[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm(tensor):\n",
    "    # create random array of floats with equal dims to tensor\n",
    "    rand = torch.rand(tensor.shape)\n",
    "    # mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
    "    mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "    # loop through each row in tensor (cannot do in parallel)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        # get indices of mask positions from mask array\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        # mask tensor\n",
    "        tensor[i, selection] = 3\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [01:49<00:00,  5.49s/it]\n"
     ]
    }
   ],
   "source": [
    "paths = [str(x) for x in Path('../../data/text/oscar_it').glob('**/*.txt')]\n",
    "# initialize lists of tensors\n",
    "input_ids = []\n",
    "mask = []\n",
    "labels = []\n",
    "# open all files, encode and add to single dataset\n",
    "for path in tqdm(paths[:50]):\n",
    "    # open the file and split into list by newline characters\n",
    "    with open(path, 'r', encoding='utf-8') as fp:\n",
    "        lines = fp.read().split('\\n')\n",
    "    # encode\n",
    "    sample = tokenizer.encode_batch(lines)\n",
    "    # convert tokens to tensor\n",
    "    labels.append(torch.tensor([x.ids for x in sample]))\n",
    "    # create attention mask tensor\n",
    "    mask.append(torch.tensor([x.attention_mask for x in sample]))\n",
    "    # mask ~15% of tokens to create inputs\n",
    "    input_ids.append(mlm(labels[-1].detach().clone()))\n",
    "# convert lists of tensors into tensors\n",
    "input_ids = torch.cat(input_ids)\n",
    "mask = torch.cat(mask)\n",
    "labels = torch.cat(labels)\n",
    "    "
   ]
  },
  {
   "source": [
    "We have 10000 tokenized sequences, each containing 512 tokens:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([200000, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "source": [
    "We can see the special tokens here, `1` is our **\\[CLS\\]** token, `2` our **\\[SEP\\]** token, `3` our **\\[MASK\\]** token, and at the end we have two `0` - or **\\[PAD\\]** - tokens:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([    1,   693, 18623,  1358,  7752,   292,  1056,   280,  7405,  6321,\n",
       "          776,   726,  2145,   280,    11, 10205,  3778,  1266,  1810,     3,\n",
       "            3,  1142, 10293,    30,     3,   267,  1340,    16,   385,  3375,\n",
       "          458,     3,  5942,   376, 25475,  2870,  1201,   391,  2691,   421,\n",
       "        17927, 16996,   739,   305,   306, 22814,     3,  7950, 17824,   980,\n",
       "          435, 18388,  1475,   275,  2597,   391,    37, 24909,   739,  2689,\n",
       "        27869,   275,  5803,   625,   770, 13459,   483,     3,   275, 12870,\n",
       "          532,    18,   680,  3867, 24138,   376,  7752, 17630, 18623,  1134,\n",
       "         8882,   269,   431,   287,     3,   483,  8041,     3,   275,  5286,\n",
       "           18,     3,   367,   275,  6161,     3, 10528,   570,  1594, 13181,\n",
       "           18,   458,    16,     3,   456,  2150, 12054,    16,   500,   317,\n",
       "         6122,     3,  3329,   570,  1594, 13181,   280, 14634,    18,   763,\n",
       "        12656,     3,  2484,  6544,     3,     3,  9106,     3,     3,  1009,\n",
       "          842,  1518, 25737,     3,   303,  3300,   306,     3,   292, 15164,\n",
       "           18,   381,   330,  2872,   343,  4722,   316,    16, 16848,   267,\n",
       "         5216,   317,  1009,   842,     3,    16,  4816,   338,   330,  2757,\n",
       "          435,  3653, 27081, 10965,    12,    39,    13, 18715,  1865,    17,\n",
       "         5580,  1056,   992,   363,    18,   360,    94,  1182,     3,     3,\n",
       "          842,   343,     3, 12863,   300,     3,  5240,    16,  5618, 10799,\n",
       "            3,     3,     3,   421, 14591, 16996,    18,     2,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "source": [
    "Next we initialize our `Dataset`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "source": [
    "And initialize the dataloader, which will load the data into the model during training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "source": [
    "And move onto building our model, we first need to create a BERT config object, which will describe which features we want to initialize our BERT model with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab set in previous notebook\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    pad_token_id=0\n",
    ")"
   ]
  },
  {
   "source": [
    "Then we import and initialize a BERT model with a language modeling head."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertLMHeadModel\n",
    "\n",
    "model = BertLMHeadModel(config)"
   ]
  },
  {
   "source": [
    "And now we move onto training. First we setup GPU/CPU usage."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = torch.utils.tensorboard.SummaryWriter()"
   ]
  },
  {
   "source": [
    "Now we move onto the training loop."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 0: 100%|██████████| 12500/12500 [1:29:47<00:00,  2.32it/s, loss=0.358]\n",
      "Epoch 1: 100%|██████████| 12500/12500 [1:22:20<00:00,  2.53it/s, loss=0.31]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # for our progress bar\n",
    "\n",
    "epochs = 2\n",
    "step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # take loss for tensorboard\n",
    "        writer.add_scalar('Loss/train', loss, step)\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./filiberto')"
   ]
  }
 ]
}